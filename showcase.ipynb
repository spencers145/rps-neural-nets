{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Welcome to the project showcase file for the Rock-Paper-Scissors-like Neural Networks project.\n",
    "\n",
    "Before continuing, make sure you read **start_here.ipynb** first.\n",
    "\n",
    "**This project focuses on the development of a way to train neural networks to fight against opponents in games that resemble rock-paper-scissors.**\n",
    "\n",
    "This is how we define \"**rock-paper-scissors-like**\":\n",
    "- Turn-based game.\n",
    "- At least 2 players (we allow more than 2).\n",
    "- Players start with a positive, whole-number score.\n",
    "    - If their score drops to 0, they are eliminated (at the end of the turn).\n",
    "- On each turn, players select one move each.\n",
    "    - There is a fixed selection of moves (e.g. rock, paper, scissors, lizard, spock).\n",
    "    - Any player can pick any move.\n",
    "- Each move is used simultaneously (e.g. everyone throws their moves at the same time in rock paper scissors).\n",
    "    - Moves target every other player, once each.\n",
    "        - e.g. imagine a game with 3 people. If two people play rock, and one plays paper, the paper hits the rock players once each (eliminating both!).\n",
    "    - The move changes the score of the target based on the target's move that turn.\n",
    "        - e.g. if you play rock, and they play scissors, they lose a point... But if you *both* play rock, they don't lose a point.\n",
    "- The game continues until either 1 or 0 players remain.\n",
    "    - If one player remains, they win.\n",
    "    - If no players remain, nobody wins (yes, this can happen. you need have at least 3 players, though).\n",
    "\n",
    "As a sanity check, we can see that rock-paper-scissors is, indeed, RPS-like: players start with a score of 1. They may choose Rock, Paper, or Scissors. They throw their moves at the same time, and players are eliminated if they play the wrong move (their score drops from 1 to 0).\n",
    "\n",
    "Another simple example (which we will make extensive use of), a best-of-three RPS game is also RPS-like: it is rock-paper-scissors, but players start with a score of 2 instead of 1 (you have to play the wrong move two times out of three to lose)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now showcase the applications of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpsnetworks.training_manager, rpsnetworks.player_templates, rpsnetworks.schema_templates, rpsnetworks.controller_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code randomly trains a neural network to play a game of rock paper scissors against an opponent who always plays rock or paper.<br>\n",
    "It will try to converge on an optimal strategy to beat its opponent the highest % of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING PARAMETERS:\n",
      "Generations: 10\n",
      "Children tested per generation: 10\n",
      "Base # of test-games per child: 10\n",
      "---------------\n",
      "Schema used: Rock Paper Scissors\n",
      "Opponent count: 1\n",
      "---------------\n",
      "RESULTS:\n",
      "Network proficiency: 1.000\n",
      "Move distribution:\n",
      "rock: 0.000\n",
      "paper: 1.000\n",
      "scissors: 0.000\n"
     ]
    }
   ],
   "source": [
    "RPS_SCHEMA = rpsnetworks.schema_templates.RockPaperScissors()\n",
    "opponents = [rpsnetworks.player_templates.mixedStrategyPlayer(\"player1\", 1, [0.5, 0.5, 0])]\n",
    "\n",
    "train_out = rpsnetworks.training_manager.trainNetwork(\n",
    "    10, 10, 10,\n",
    "    [2, 2, 3], rpsnetworks.player_templates.basicNetworkPlayer, 1,\n",
    "    opponents, RPS_SCHEMA, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block above also outputs the results.<br>\n",
    "Network proficiency is the proportion of games the network won against a final test of 10,000 games against its opponent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train one to play against two players at once:<br>\n",
    "The first player always plays rock, the second always plays paper.<br>\n",
    "Both our network and the opponents start with 3 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING PARAMETERS:\n",
      "Generations: 10\n",
      "Children tested per generation: 10\n",
      "Base # of test-games per child: 10\n",
      "---------------\n",
      "Schema used: Rock Paper Scissors\n",
      "Opponent count: 2\n",
      "---------------\n",
      "RESULTS:\n",
      "Network proficiency: 0.976\n",
      "Move distribution:\n",
      "rock: 0.000\n",
      "paper: 0.709\n",
      "scissors: 0.291\n"
     ]
    }
   ],
   "source": [
    "opponents = [\n",
    "    rpsnetworks.player_templates.mixedStrategyPlayer(\"player1\", 3, [1, 0, 0]),\n",
    "    rpsnetworks.player_templates.mixedStrategyPlayer(\"player2\", 3, [0, 1, 0])\n",
    "]\n",
    "\n",
    "train_out = rpsnetworks.training_manager.trainNetwork(\n",
    "    10, 10, 10,\n",
    "    [2, 2, 3], rpsnetworks.player_templates.basicNetworkPlayer, 3,\n",
    "    opponents, RPS_SCHEMA, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not perfect.<br>\n",
    "We can swap to another model that is aware of both its health and its opponents.<br>\n",
    "This will allow the network to develop more a advanced strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING PARAMETERS:\n",
      "Generations: 10\n",
      "Children tested per generation: 10\n",
      "Base # of test-games per child: 10\n",
      "---------------\n",
      "Schema used: Rock Paper Scissors\n",
      "Opponent count: 2\n",
      "---------------\n",
      "RESULTS:\n",
      "Network proficiency: 1.000\n",
      "Move distribution:\n",
      "rock: 0.000\n",
      "paper: 0.250\n",
      "scissors: 0.750\n"
     ]
    }
   ],
   "source": [
    "opponents = [\n",
    "    rpsnetworks.player_templates.mixedStrategyPlayer(\"player1\", 3, [1, 0, 0]),\n",
    "    rpsnetworks.player_templates.mixedStrategyPlayer(\"player2\", 3, [0, 1, 0])\n",
    "]\n",
    "\n",
    "train_out = rpsnetworks.training_manager.trainNetwork(\n",
    "    10, 10, 10,\n",
    "    [4, 2, 3], rpsnetworks.player_templates.playerAwareNetworkPlayer, 3,\n",
    "    opponents, RPS_SCHEMA, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, the network will train itself here to play paper until the first opponent is defeated. Then, it will play scissors until the second opponent is defeated.\n",
    "\n",
    "However, it sometimes develops...alternative strategies. For example, it's possible to sometimes play rock and still win 100% of battles.<br>\n",
    "The fitness function does not punish these silly moves. We are optimizing only for a winning strategy, so there is no need. Introducing extra complexity into the scoring algorithm also reduces performance, and risks introducing biases that make for suboptimal training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more nuanced example:<br>\n",
    "Player 1 plays Rock 50% of the time, Paper 25%, and Scissors 25%.<br>\n",
    "Player 2 plays Paper 50% of the time, and Scissors 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING PARAMETERS:\n",
      "Generations: 10\n",
      "Children tested per generation: 40\n",
      "Base # of test-games per child: 20\n",
      "---------------\n",
      "Schema used: Rock Paper Scissors\n",
      "Opponent count: 2\n",
      "---------------\n",
      "RESULTS:\n",
      "Network proficiency: 0.644\n",
      "Move distribution:\n",
      "rock: 0.000\n",
      "paper: 0.253\n",
      "scissors: 0.747\n"
     ]
    }
   ],
   "source": [
    "opponents = [\n",
    "    rpsnetworks.player_templates.mixedStrategyPlayer(\"player1\", 3, [0.5, 0.25, 0.25]),\n",
    "    rpsnetworks.player_templates.mixedStrategyPlayer(\"player2\", 3, [0, 0.5, 0.5])\n",
    "]\n",
    "\n",
    "train_out = rpsnetworks.training_manager.trainNetwork(\n",
    "    10, 40, 20,\n",
    "    [4, 2, 3], rpsnetworks.player_templates.playerAwareNetworkPlayer, 3,\n",
    "    opponents, RPS_SCHEMA, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal strategy here is harder to find - and there is no strategy for a 100% win rate.<br>\n",
    "Training results will vary. Typically, network proficiency is greater than 60% - but it will sometimes get stuck in bad strategies, like always playing scissors.\n",
    "\n",
    "Care has been taken to design the training algorithm to reduce the frequency of these sorts of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rock paper scissors is an RPS-like, but many other games are, as well.\n",
    "\n",
    "We will play a custom game:<br>\n",
    "There are 5 moves. You can *Attack*, *Defend*, *Special*, *Deflect*, or *Heal*.<br>\n",
    "If you attack, and your opponent does not defend, they lose 1 point.<br>\n",
    "If you defend, and your opponent uses special, both of you lose 1 point.<br>\n",
    "If you use special, and your opponent uses attack, special, or heal, they lose 2 points.<br>\n",
    "If you deflect, and your opponent uses special, they lose 2 points.<br>\n",
    "If you heal, you gain 1 point on top of any losses inflicted by the opponent that turn.<br>\n",
    "The same goes for your opponent for all these rules. (e.g. if you both use special, then you both lose 2 points)<br>\n",
    "This is an RPS-like. It's a more intricate system to train AI on, and helped expose shortcomings in the training process while developing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An opponent who only attacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING PARAMETERS:\n",
      "Generations: 10\n",
      "Children tested per generation: 10\n",
      "Base # of test-games per child: 10\n",
      "---------------\n",
      "Schema used: RPG Fascimile\n",
      "Opponent count: 1\n",
      "---------------\n",
      "RESULTS:\n",
      "Network proficiency: 1.000\n",
      "Move distribution:\n",
      "attack: 0.000\n",
      "defend: 0.000\n",
      "special: 0.308\n",
      "deflect: 0.000\n",
      "heal: 0.692\n"
     ]
    }
   ],
   "source": [
    "RPG_SCHEMA = rpsnetworks.schema_templates.RPGFascimile()\n",
    "\n",
    "opponents = [\n",
    "    rpsnetworks.player_templates.mixedStrategyPlayer(\"player1\", 4, [1, 0, 0, 0, 0]),\n",
    "]\n",
    "\n",
    "train_out = rpsnetworks.training_manager.trainNetwork(\n",
    "    10, 10, 10,\n",
    "    [2, 2, 5], rpsnetworks.player_templates.basicNetworkPlayer, 4,\n",
    "    opponents, RPG_SCHEMA, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically the training process discovers a strategy involving frequent use of the special move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a network aware of its opponent's health against an opponent who attacks 50% of the time, specials 25% and deflects 25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING PARAMETERS:\n",
      "Generations: 10\n",
      "Children tested per generation: 40\n",
      "Base # of test-games per child: 30\n",
      "---------------\n",
      "Schema used: RPG Fascimile\n",
      "Opponent count: 1\n",
      "---------------\n",
      "RESULTS:\n",
      "Network proficiency: 0.791\n",
      "Move distribution:\n",
      "attack: 0.000\n",
      "defend: 0.604\n",
      "special: 0.000\n",
      "deflect: 0.000\n",
      "heal: 0.396\n"
     ]
    }
   ],
   "source": [
    "opponents = [\n",
    "    rpsnetworks.player_templates.mixedStrategyPlayer(\"player1\", 4, [0.50, 0, 0.25, 0.25, 0]),\n",
    "]\n",
    "\n",
    "train_out = rpsnetworks.training_manager.trainNetwork(\n",
    "    10, 40, 30,\n",
    "    [3, 2, 5], rpsnetworks.player_templates.playerAwareNetworkPlayer, 4,\n",
    "    opponents, RPG_SCHEMA, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many times, the training process discovers the following simple, but effective strategy:<br>\n",
    "*Heal* until I have more health than my opponent.<br>\n",
    "*Defend*, and the recoil from my opponent using special will eventually cause me to win."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train a network aware of its opponent's and its own health against an opponent who is equally likely to use any move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING PARAMETERS:\n",
      "Generations: 10\n",
      "Children tested per generation: 20\n",
      "Base # of test-games per child: 20\n",
      "---------------\n",
      "Schema used: RPG Fascimile\n",
      "Opponent count: 1\n",
      "---------------\n",
      "RESULTS:\n",
      "Network proficiency: 0.797\n",
      "Move distribution:\n",
      "attack: 0.000\n",
      "defend: 0.000\n",
      "special: 0.300\n",
      "deflect: 0.000\n",
      "heal: 0.700\n"
     ]
    }
   ],
   "source": [
    "opponents = [\n",
    "    rpsnetworks.player_templates.mixedStrategyPlayer(\"player1\", 4, [0.2, 0.2, 0.2, 0.2, 0.2]),\n",
    "]\n",
    "\n",
    "train_out = rpsnetworks.training_manager.trainNetwork(\n",
    "    10, 20, 20,\n",
    "    [3, 3, 5], rpsnetworks.player_templates.playerAwareNetworkPlayer, 4,\n",
    "    opponents, RPG_SCHEMA, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically the training process discovers a strategy pretty similar to the last time. It heals until it has the HP advantage (something between 1 and 3, depending on the network), then attacks or specials repeatedly. If it loses the HP advantage, it heals until it gets it back.\n",
    "\n",
    "Some networks also learn that, while they have the advantage and their opponent is low, to use special.\n",
    "\n",
    "The network might be defeated if the opponent uses special enough times while the network is trying to get the health advantage - and can also be defeated if it gets greedy (uses special while the opponent is at low health), and the opponent happens to deflect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train against an opponent who heals 50% of the time, and does another random move the other 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING PARAMETERS:\n",
      "Generations: 10\n",
      "Children tested per generation: 20\n",
      "Base # of test-games per child: 20\n",
      "---------------\n",
      "Schema used: RPG Fascimile\n",
      "Opponent count: 1\n",
      "---------------\n",
      "RESULTS:\n",
      "Network proficiency: 0.888\n",
      "Move distribution:\n",
      "attack: 0.000\n",
      "defend: 0.000\n",
      "special: 0.435\n",
      "deflect: 0.000\n",
      "heal: 0.565\n"
     ]
    }
   ],
   "source": [
    "opponents = [\n",
    "    rpsnetworks.player_templates.mixedStrategyPlayer(\"player1\", 4, [0.125, 0.125, 0.125, 0.125, 0.5]),\n",
    "]\n",
    "\n",
    "train_out = rpsnetworks.training_manager.trainNetwork(\n",
    "    10, 20, 20,\n",
    "    [3, 3, 5], rpsnetworks.player_templates.playerAwareNetworkPlayer, 4,\n",
    "    opponents, RPG_SCHEMA, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what happens when we put a pre-programmed, strategic AI up against our neural network.<br>\n",
    "This AI likes to attack and special when it has the advantage, deflect and heal when at a disadvantage, and also heal if it has the same health as any of its opponents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING PARAMETERS:\n",
      "Generations: 15\n",
      "Children tested per generation: 64\n",
      "Base # of test-games per child: 20\n",
      "---------------\n",
      "Schema used: RPG Fascimile\n",
      "Opponent count: 1\n",
      "---------------\n",
      "RESULTS:\n",
      "Network proficiency: 0.597\n",
      "Move distribution:\n",
      "attack: 0.207\n",
      "defend: 0.025\n",
      "special: 0.172\n",
      "deflect: 0.000\n",
      "heal: 0.596\n"
     ]
    }
   ],
   "source": [
    "opponents = [\n",
    "    rpsnetworks.player_templates.Player(\"player1\", 4,\n",
    "                                        rpsnetworks.controller_templates.customStrategy1Controller(\"player1\", \"cool_strat\", [0.2, 0.2, 0.2, 0.2, 0.2]))\n",
    "]\n",
    "\n",
    "train_out = rpsnetworks.training_manager.trainNetwork(\n",
    "    15, 64, 20,\n",
    "    [3, 3, 5], rpsnetworks.player_templates.playerAwareNetworkPlayer, 4,\n",
    "    opponents, RPG_SCHEMA, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network can end up training itself in a wide variety of ways. However, it struggles to break a 65% win rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING PARAMETERS:\n",
      "Generations: 15\n",
      "Children tested per generation: 64\n",
      "Base # of test-games per child: 20\n",
      "---------------\n",
      "Schema used: RPG Fascimile\n",
      "Opponent count: 2\n",
      "---------------\n",
      "RESULTS:\n",
      "Network proficiency: 0.916\n",
      "Move distribution:\n",
      "attack: 0.353\n",
      "defend: 0.008\n",
      "special: 0.015\n",
      "deflect: 0.004\n",
      "heal: 0.619\n"
     ]
    }
   ],
   "source": [
    "opponents = [\n",
    "    rpsnetworks.player_templates.Player(\"player1\", 4,\n",
    "                                        rpsnetworks.controller_templates.customStrategy1Controller(\"player1\", \"cool_strat\", [0.2, 0.2, 0.2, 0.2, 0.2])),\n",
    "    rpsnetworks.player_templates.Player(\"player2\", 4,\n",
    "                                        rpsnetworks.controller_templates.customStrategy1Controller(\"player2\", \"cool_strat\", [0.2, 0.2, 0.2, 0.2, 0.2]))\n",
    "]\n",
    "\n",
    "train_out = rpsnetworks.training_manager.trainNetwork(\n",
    "    15, 64, 20,\n",
    "    [4, 3, 5], rpsnetworks.player_templates.playerAwareNetworkPlayer, 4,\n",
    "    opponents, RPG_SCHEMA, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, the neural network has a much easier time winning if it's up against 2 players. (win rates of >85%)<br>\n",
    "One strategy is as follows: it sits out the fight for a bit by healing at the start, waiting to have a good advantage over the other two players, who are lowering each others' healths while preserving its own strength.<br>\n",
    "Once it has the advantage, it attacks repeatedly - healing if necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
